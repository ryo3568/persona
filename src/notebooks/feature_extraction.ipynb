{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle \n",
    "import os \n",
    "import glob\n",
    "import pympi.Elan \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "from transformers import BertModel \n",
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "with open('../../data/Hazumi_features/Hazumi1911_features_bert_norm.pkl', mode='rb') as f:\n",
    "    SS_ternary, TS_ternary, sentiment, third_sentiment, persona, third_persona, text, audio,\\\n",
    "    visual, vid = pickle.load(f, encoding='utf-8')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# オークナイザーの読み込み\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    ")\n",
    "\n",
    "# 学習済みモデルの読み込み\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\").to(device)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\" \n",
    "    フィラー記号(F *)，|の除去\n",
    "    \"\"\"\n",
    "    text = re.sub('\\([^)]*\\)', '', text)\n",
    "    text = text.replace('|', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "def embedding(sentences):\n",
    "    # 前処理\n",
    "\n",
    "    sentences = [preprocess(text) for text in sentences]\n",
    "\n",
    "    # BERTトークン化\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        sentences, padding=True, add_special_tokens=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # BERTトークンID列を抽出\n",
    "    input_ids = torch.tensor(encoded[\"input_ids\"], device=device) \n",
    "\n",
    "    # BERTの最大許容トークン数が512なので超える場合は切り詰める\n",
    "    input_ids = input_ids[:, :512] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids) \n",
    "\n",
    "    # 最終層の隠れ状態ベクトルを取得\n",
    "    last_hidden_states = outputs[0] \n",
    "\n",
    "    # [CLS]トークンの単語ベクトルを抽出\n",
    "    vecs = last_hidden_states[:, 0, :]\n",
    "\n",
    "    return vecs.tolist()\n",
    "\n",
    "\n",
    "# 本人にアノテーションされた性格特性スコアの算出\n",
    "def calc_persona(filename, normalized=False):\n",
    "    df = pd.read_excel('../../data/Hazumi1911/questionnaire/1911questionnaires.xlsx', sheet_name=4, index_col=0, header=1)\n",
    "    data = df.loc[filename, :].values.tolist()\n",
    "    res = [data[0]+(8-data[5]), (8-data[1])+data[6], data[2]+(8-data[7]), data[3]+(8-data[8]), data[4]+(8-data[9])]\n",
    "    if normalized:\n",
    "        return [(i-2)/12 for i in res]\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "# 第三者にアノテーションされた性格特性スコアの算出\n",
    "def calc_thirdpersona(filename, normalized=False):\n",
    "    df = pd.read_excel('../../data/Hazumi1911/questionnaire/220818thirdbigfive-Hazumi1911.xlsx', sheet_name=5, header=1, index_col=0)\n",
    "    data = df.loc[filename].values.tolist()\n",
    "    res = [data[5], data[13], data[21], data[29], data[37]]\n",
    "    if normalized:\n",
    "        return [(i-2)/12 for i in res]\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def eaf_to_df( eaf: pympi.Elan.Eaf ) -> pd.DataFrame:\n",
    "    tier_names = list( eaf.tiers.keys() )\n",
    "\n",
    "    def timeslotid_to_time( timeslotid: str ) -> float:\n",
    "        return eaf.timeslots[ timeslotid ] / 1000\n",
    "\n",
    "    def parse( tier_name: str, tier: dict ) -> pd.DataFrame:\n",
    "        values = [ (key,) + value[:-1] for key, value in tier.items() ]\n",
    "        df = pd.DataFrame( values, columns=[ \"id\", \"start\", \"end\", \"transcription\"] )\n",
    "\n",
    "        df[\"start\"] = df[\"start\"].apply( timeslotid_to_time )\n",
    "        df[\"end\"] = df[\"end\"].apply( timeslotid_to_time )\n",
    "        df[\"ID\"] = df.apply( lambda x: f\"{tier_name}-{x.name}\", axis=1 )\n",
    "        df = df.reindex( columns=[\"ID\", \"start\", \"end\", \"transcription\"] )\n",
    "\n",
    "        return df\n",
    "\n",
    "    dfs = [ parse(tier_name=name, tier=eaf.tiers[name][0]) for name in tier_names ]\n",
    "    df = pd.concat( dfs )\n",
    "    df = df.sort_values( \"start\" )\n",
    "    df = df.reset_index( drop=True )\n",
    "    return df\n",
    "\n",
    "def extract_sentence(filename, start):\n",
    "    res = []\n",
    "    src = '../../data/Hazumi1911/elan/' + filename + '.eaf' \n",
    "\n",
    "    eaf = pympi.Elan.Eaf(src) \n",
    "    df = eaf_to_df(eaf) \n",
    "\n",
    "    df['start'] = (df['start'] * 1000).astype(int)\n",
    "\n",
    "    for time in start:\n",
    "        sentence = df[(df['start'] == time) & (df['ID'].str.contains('user'))]['transcription'].values.tolist()\n",
    "        if len(sentence) == 0:\n",
    "            sentence = ['']\n",
    "        sentence = embedding(sentence)\n",
    "        res.append(sentence[0])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ダンプファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = {}\n",
    "text = {}\n",
    "visual = {} \n",
    "\n",
    "third_persona = {}\n",
    "persona = {}\n",
    "TS_ternary = {}\n",
    "SS_ternary = {}\n",
    "third_sentiment = {}\n",
    "sentiment = {}\n",
    "\n",
    "vid = []\n",
    "\n",
    "path = '../../data/Hazumi1911/dumpfiles/*'\n",
    "\n",
    "files = glob.glob(path)\n",
    "\n",
    "for file_path in sorted(files):\n",
    "    filename = os.path.basename(file_path).split('.', 1)[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    vid.append(filename)\n",
    "    text[filename] = df.loc[:, 'word#0001':'su'].values.tolist()\n",
    "    audio[filename] = df.loc[:, 'pcm_RMSenergy_sma_max':'F0_sma_de_kurtosis'].values.tolist()\n",
    "    visual[filename] = df.loc[:, '17_acceleration_max':'AU45_c_mean'].values.tolist()\n",
    "\n",
    "    persona[filename] = calc_persona(filename)\n",
    "    third_persona[filename] = calc_thirdpersona(filename)\n",
    "    TS_ternary[filename] = df.loc[:, 'TS_ternary'].values.tolist()\n",
    "    SS_ternary[filename] = df.loc[:, 'SS_ternary'].values.tolist()\n",
    "    third_sentiment[filename] = df.loc[:, 'TS1':'TS5'].mean(axis='columns').values.tolist()\n",
    "    sentiment[filename] = df.loc[:, 'SS'].values.tolist()\n",
    "\n",
    "# ファイル書き込み\n",
    "with open('../../data/Hazumi_features/Hazumi1911_features.pkl', mode='wb') as f:\n",
    "    pickle.dump((SS_ternary, TS_ternary, sentiment, third_sentiment, persona, third_persona, text, audio, visual, vid), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 言語特徴量をBERTから抽出\n",
    "audio = {}\n",
    "text = {}\n",
    "visual = {} \n",
    "\n",
    "third_persona = {}\n",
    "persona = {}\n",
    "TS_ternary = {}\n",
    "SS_ternary = {}\n",
    "third_sentiment = {}\n",
    "sentiment = {}\n",
    "\n",
    "vid = []\n",
    "\n",
    "path = '../../data/Hazumi1911/dumpfiles/*'\n",
    "\n",
    "files = glob.glob(path)\n",
    "\n",
    "for file_path in sorted(files):\n",
    "    filename = os.path.basename(file_path).split('.', 1)[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    start = df['start(exchange)[ms]'].values.tolist()\n",
    "\n",
    "    vid.append(filename)\n",
    "    text[filename] = extract_sentence(filename, start)\n",
    "    audio[filename] = df.loc[:, 'pcm_RMSenergy_sma_max':'F0_sma_de_kurtosis'].values.tolist()\n",
    "    visual[filename] = df.loc[:, '17_acceleration_max':'AU45_c_mean'].values.tolist()\n",
    "\n",
    "    persona[filename] = calc_persona(filename)\n",
    "    third_persona[filename] = calc_thirdpersona(filename)\n",
    "    TS_ternary[filename] = df.loc[:, 'TS_ternary'].values.tolist()\n",
    "    SS_ternary[filename] = df.loc[:, 'SS_ternary'].values.tolist()\n",
    "    third_sentiment[filename] = df.loc[:, 'TS1':'TS5'].mean(axis='columns').values.tolist()\n",
    "    sentiment[filename] = df.loc[:, 'SS'].values.tolist()\n",
    "\n",
    "# ファイル書き込み\n",
    "with open('../../data/Hazumi_features/Hazumi1911_features_bert.pkl', mode='wb') as f:\n",
    "\n",
    "    pickle.dump((SS_ternary, TS_ternary, sentiment, third_sentiment, persona, third_persona, text, audio, visual, vid), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 言語特徴量をBERTから抽出 && 性格特性スコア正規化\n",
    "audio = {}\n",
    "text = {}\n",
    "visual = {} \n",
    "\n",
    "third_persona = {}\n",
    "persona = {}\n",
    "TS_ternary = {}\n",
    "SS_ternary = {}\n",
    "third_sentiment = {}\n",
    "sentiment = {}\n",
    "\n",
    "vid = []\n",
    "\n",
    "path = '../../data/Hazumi1911/dumpfiles/*'\n",
    "\n",
    "files = glob.glob(path)\n",
    "\n",
    "for file_path in sorted(files):\n",
    "    filename = os.path.basename(file_path).split('.', 1)[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    start = df['start(exchange)[ms]'].values.tolist()\n",
    "\n",
    "    vid.append(filename)\n",
    "    text[filename] = extract_sentence(filename, start)\n",
    "    audio[filename] = df.loc[:, 'pcm_RMSenergy_sma_max':'F0_sma_de_kurtosis'].values.tolist()\n",
    "    visual[filename] = df.loc[:, '17_acceleration_max':'AU45_c_mean'].values.tolist()\n",
    "\n",
    "    persona[filename] = calc_persona(filename, True)\n",
    "    third_persona[filename] = calc_thirdpersona(filename, True)\n",
    "    TS_ternary[filename] = df.loc[:, 'TS_ternary'].values.tolist()\n",
    "    SS_ternary[filename] = df.loc[:, 'SS_ternary'].values.tolist()\n",
    "    third_sentiment[filename] = df.loc[:, 'TS1':'TS5'].mean(axis='columns').values.tolist()\n",
    "    sentiment[filename] = df.loc[:, 'SS'].values.tolist()\n",
    "\n",
    "# ファイル書き込み\n",
    "with open('../../data/Hazumi_features/Hazumi1911_features_bert_norm.pkl', mode='wb') as f:\n",
    "\n",
    "    pickle.dump((SS_ternary, TS_ternary, sentiment, third_sentiment, persona, third_persona, text, audio, visual, vid), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2368, 1224)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = []\n",
    "for x in vid:\n",
    "    Text = np.array(text[x]) \n",
    "    Audio = np.array(audio[x]) \n",
    "    Visual = np.array(visual[x]) \n",
    "\n",
    "    seq_len = Text.shape[0]\n",
    "    \n",
    "    Ternary = np.array(TS_ternary[x])\n",
    "    Ternary = Ternary[:, np.newaxis].astype(int)\n",
    "    Persona = np.array(third_persona[x])\n",
    "    Persona = np.tile(Persona, (seq_len, 1))\n",
    "\n",
    "    data = np.concatenate([Text, Audio, Visual, Ternary, Persona], 1) \n",
    "\n",
    "    if x == '1911F2001':\n",
    "        test = data\n",
    "    else:\n",
    "        train.append(data)\n",
    "\n",
    "train = np.concatenate(train)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, neu, pos = 0, 0, 0\n",
    "for value in TS_ternary.values():\n",
    "    for data in value:\n",
    "        if data == 0:\n",
    "            neg += 1\n",
    "        elif data == 1:\n",
    "            neu += 1\n",
    "        else:\n",
    "            pos += 1\n",
    "sum = neg + neu + pos\n",
    "print('-----TS_ternary-----')\n",
    "print(neg, neu, pos, sum)\n",
    "majo = max([neg/sum, neu/sum, pos/sum])\n",
    "print(f'マジョリティベースライン：{majo:.3}')\n",
    "\n",
    "neg, neu, pos = 0, 0, 0\n",
    "for value in SS_ternary.values():\n",
    "    for data in value:\n",
    "        if data == 0:\n",
    "            neg += 1\n",
    "        elif data == 1:\n",
    "            neu += 1\n",
    "        else:\n",
    "            pos += 1\n",
    "sum = neg + neu + pos\n",
    "print('-----SS_ternary-----')\n",
    "print(neg, neu, pos, sum)\n",
    "majo = max([neg/sum, neu/sum, pos/sum])\n",
    "print(f'マジョリティベースライン(SS_ternary)：{majo:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visual['1911F2001'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('persona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66a55e160c955a136bd04b9ee0e97041b96c673504f5e57fef660b8b6eac7e97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
